{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow8aZiGNnPGG"
      },
      "source": [
        "# A Short MACE Tutorial\n",
        "## Ilyes Batatia \n",
        "\n",
        "Hot link to Google Colab version: https://colab.research.google.com/drive/1D6EtMUjQPey_GkuxUAbPgld6_9ibIa-V?authuser=0&pli=1#scrollTo=X2XNYxlFHEKR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlFX3zUWoMdL"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This is a short tutorial for MACE, a highly accurate and efficient ML interatomic potential.\n",
        "Please read the associated [paper](https://arxiv.org/pdf/2206.07697.pdf).\n",
        "The reference implementation is available [here](https://github.com/ACEsuit/mace)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rtvmWR_nZgW"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Icx3drmDnydW",
        "outputId": "da8e57d6-2222-475e-b00a-0fadb8e02931"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting e3nn==0.4.4\n",
            "  Downloading e3nn-0.4.4-py3-none-any.whl (387 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m387.7/387.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt_einsum in /usr/local/lib/python3.10/dist-packages (3.3.0)\n",
            "Collecting ase\n",
            "  Downloading ase-3.22.1-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_ema\n",
            "  Downloading torch_ema-0.3-py3-none-any.whl (5.5 kB)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (3.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from e3nn==0.4.4) (1.12)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from e3nn==0.4.4) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from e3nn==0.4.4) (2.2.1+cu121)\n",
            "Collecting opt-einsum-fx>=0.1.4 (from e3nn==0.4.4)\n",
            "  Downloading opt_einsum_fx-0.1.4-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.10/dist-packages (from opt_einsum) (1.25.2)\n",
            "Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from ase) (3.7.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable) (0.2.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn==0.4.4) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn==0.4.4) (4.10.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn==0.4.4) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn==0.4.4) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn==0.4.4) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->e3nn==0.4.4)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->e3nn==0.4.4)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->e3nn==0.4.4)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->e3nn==0.4.4)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->e3nn==0.4.4)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->e3nn==0.4.4)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->e3nn==0.4.4)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->e3nn==0.4.4)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->e3nn==0.4.4)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.8.0->e3nn==0.4.4)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->e3nn==0.4.4)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn==0.4.4) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->e3nn==0.4.4)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->e3nn==0.4.4) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.0->ase) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->e3nn==0.4.4) (2.1.5)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ase, torch_ema, opt-einsum-fx, e3nn\n",
            "Successfully installed ase-3.22.1 e3nn-0.4.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 opt-einsum-fx-0.1.4 torch_ema-0.3\n",
            "Cloning into 'mace'...\n",
            "remote: Enumerating objects: 77, done.\u001b[K\n",
            "remote: Counting objects: 100% (77/77), done.\u001b[K\n",
            "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
            "remote: Total 77 (delta 0), reused 28 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (77/77), 50.97 MiB | 29.09 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install e3nn==0.4.4 opt_einsum ase torch_ema prettytable\n",
        "\n",
        "# Clone MACE\n",
        "!git clone --depth 1 https://github.com/ACEsuit/mace.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "X2XNYxlFHEKR",
        "outputId": "ba0d3b8a-568f-48fe-dc02-fe275a4918a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing ./mace\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.12 in /usr/local/lib/python3.10/dist-packages (from mace-torch==0.3.4) (2.2.1+cu121)\n",
            "Requirement already satisfied: e3nn==0.4.4 in /usr/local/lib/python3.10/dist-packages (from mace-torch==0.3.4) (0.4.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mace-torch==0.3.4) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from mace-torch==0.3.4) (3.3.0)\n",
            "Requirement already satisfied: ase in /usr/local/lib/python3.10/dist-packages (from mace-torch==0.3.4) (3.22.1)\n",
            "Requirement already satisfied: torch-ema in /usr/local/lib/python3.10/dist-packages (from mace-torch==0.3.4) (0.3)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (from mace-torch==0.3.4) (3.10.0)\n",
            "Collecting matscipy (from mace-torch==0.3.4)\n",
            "  Downloading matscipy-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (438 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.9/438.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mace-torch==0.3.4) (3.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from mace-torch==0.3.4) (1.5.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from e3nn==0.4.4->mace-torch==0.3.4) (1.12)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from e3nn==0.4.4->mace-torch==0.3.4) (1.11.4)\n",
            "Requirement already satisfied: opt-einsum-fx>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from e3nn==0.4.4->mace-torch==0.3.4) (0.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->mace-torch==0.3.4) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->mace-torch==0.3.4) (4.10.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->mace-torch==0.3.4) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->mace-torch==0.3.4) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->mace-torch==0.3.4) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->mace-torch==0.3.4) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->mace-torch==0.3.4) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->mace-torch==0.3.4) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->mace-torch==0.3.4) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->mace-torch==0.3.4) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->mace-torch==0.3.4) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->mace-torch==0.3.4) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->mace-torch==0.3.4) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->mace-torch==0.3.4) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->mace-torch==0.3.4) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->mace-torch==0.3.4) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->mace-torch==0.3.4) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.12->mace-torch==0.3.4) (12.4.99)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mace-torch==0.3.4) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mace-torch==0.3.4) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mace-torch==0.3.4) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mace-torch==0.3.4) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mace-torch==0.3.4) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mace-torch==0.3.4) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mace-torch==0.3.4) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mace-torch==0.3.4) (2.8.2)\n",
            "Collecting looseversion (from matscipy->mace-torch==0.3.4)\n",
            "  Downloading looseversion-1.3.0-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->mace-torch==0.3.4) (2023.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable->mace-torch==0.3.4) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mace-torch==0.3.4) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12->mace-torch==0.3.4) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->e3nn==0.4.4->mace-torch==0.3.4) (1.3.0)\n",
            "Building wheels for collected packages: mace-torch\n",
            "  Building wheel for mace-torch (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mace-torch: filename=mace_torch-0.3.4-py3-none-any.whl size=87188 sha256=09450eae8a778c0bdccf4f218e2f4af17123a0bc35d10bcb3de28b0fa4fa7ef9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-up82ga0y/wheels/df/5f/32/ef59561725170a81c728fd01c75e56a9ee83bad6da485fc6a5\n",
            "Successfully built mace-torch\n",
            "Installing collected packages: looseversion, matscipy, mace-torch\n",
            "Successfully installed looseversion-1.3.0 mace-torch-0.3.4 matscipy-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install mace/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdG2dLNXDhVN"
      },
      "source": [
        "**Note:** Make sure to enable GPU: Runtime --> Change runtime type to GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I75fyRko1TnJ"
      },
      "source": [
        "## Loading Data\n",
        "The data files used to train the MACE model have to be in `extxyz` format.\n",
        "In this tutorial, we use the 3BPA dataset consisting of 500 configurations sampled a 300K with DFT.\n",
        "The energies are in eV and forces in eV/A."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AlpRFJqdn60u",
        "outputId": "0f7631fa-169b-4ad3-de77-7c03a281241e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'BOTNet-datasets'...\n",
            "remote: Enumerating objects: 57, done.\u001b[K\n",
            "remote: Counting objects: 100% (57/57), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 57 (delta 13), reused 37 (delta 7), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (57/57), 28.73 MiB | 20.03 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/davkovacs/BOTNet-datasets.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pdZfwzhXzxD8",
        "outputId": "c8bbffff-d14c-4cf1-be65-ce9626c84eea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iso_atoms.xyz  test_1200K.xyz  test_600K.xyz  train_300K.xyz\n",
            "README.md      test_300K.xyz   test_dih.xyz   train_mixedT.xyz\n"
          ]
        }
      ],
      "source": [
        "!ls BOTNet-datasets/dataset_3BPA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKVohiQJ1P3R"
      },
      "source": [
        "## Training\n",
        "\n",
        "To train a MACE model you can specify the training file with the `--train_file` flag. The validation set can either be specified as a separate file using the `--valid_file` keyword, or it can be specified as a fraction of the training set using the `--valid_fraction` keyword. It is also possible to provide a test set that only gets evaluated at the end of the training using the `--test_file` keyword. If you want to compute the RMSE for different parts of the training set separately, specify the `config_type` keyword in the `info` dict of the configurations.\n",
        "\n",
        "When parsing the data files the energies are read using the keyword `energy` and the forces using the keyword `forces`. To change that, specify the `--energy_key` and `--forces_key`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_NWojkilmE1"
      },
      "source": [
        "For illustration, we create a very small model with 16 invariant messages specified by `hidden_irreps='16x0e'`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Z10787RE1N8T",
        "outputId": "31f9be75-4f69-487a-c16b-cb7c0b414b42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-04-02 10:00:25.893 INFO: MACE version: 0.3.4\n",
            "2024-04-02 10:00:25.895 INFO: Configuration: Namespace(name='MACE_model', seed=123, log_dir='logs', model_dir='.', checkpoints_dir='checkpoints', results_dir='results', downloads_dir='downloads', device='cpu', default_dtype='float32', log_level='INFO', error_table='PerAtomRMSE', model='ScaleShiftMACE', r_max=4.0, radial_type='bessel', num_radial_basis=8, num_cutoff_basis=5, interaction='RealAgnosticResidualInteractionBlock', interaction_first='RealAgnosticResidualInteractionBlock', max_ell=3, correlation=3, num_interactions=2, MLP_irreps='16x0e', radial_MLP='[64, 64, 64]', hidden_irreps='32x0e', num_channels=None, max_L=None, gate='silu', scaling='rms_forces_scaling', avg_num_neighbors=1, compute_avg_num_neighbors=True, compute_stress=False, compute_forces=True, train_file='BOTNet-datasets/dataset_3BPA/train_300K.xyz', valid_file=None, valid_fraction=0.05, test_file='BOTNet-datasets/dataset_3BPA/test_300K.xyz', E0s='{1:-13.663181292231226, 6:-1029.2809654211628, 7:-1484.1187695035828, 8:-2042.0330099956639}', energy_key='energy', forces_key='forces', virials_key='virials', stress_key='stress', dipole_key='dipole', charges_key='charges', loss='weighted', forces_weight=100.0, swa_forces_weight=100.0, energy_weight=1.0, swa_energy_weight=1000.0, virials_weight=1.0, swa_virials_weight=10.0, stress_weight=1.0, swa_stress_weight=10.0, dipole_weight=1.0, swa_dipole_weight=1.0, config_type_weights='{\"Default\":1.0}', huber_delta=0.01, optimizer='adam', batch_size=20, valid_batch_size=10, lr=0.01, swa_lr=0.001, weight_decay=5e-07, amsgrad=True, scheduler='ReduceLROnPlateau', lr_factor=0.8, scheduler_patience=50, lr_scheduler_gamma=0.9993, swa=True, start_swa=None, ema=True, ema_decay=0.99, max_num_epochs=2, patience=2048, eval_interval=2, keep_checkpoints=False, restart_latest=False, save_cpu=False, clip_grad=10.0, wandb=False, wandb_project='', wandb_entity='', wandb_name='', wandb_log_hypers=['num_channels', 'max_L', 'correlation', 'lr', 'swa_lr', 'weight_decay', 'batch_size', 'max_num_epochs', 'start_swa', 'energy_weight', 'forces_weight'])\n",
            "2024-04-02 10:00:25.896 INFO: Using CPU\n",
            "2024-04-02 10:00:26.394 INFO: Loaded 500 training configurations from 'BOTNet-datasets/dataset_3BPA/train_300K.xyz'\n",
            "2024-04-02 10:00:26.395 INFO: Using random 5.0% of training set for validation\n",
            "2024-04-02 10:00:27.299 INFO: Loaded 1669 test configurations from 'BOTNet-datasets/dataset_3BPA/test_300K.xyz'\n",
            "2024-04-02 10:00:27.300 INFO: Total number of configurations: train=475, valid=25, tests=[Default: 1669]\n",
            "2024-04-02 10:00:27.304 INFO: AtomicNumberTable: (1, 6, 7, 8)\n",
            "2024-04-02 10:00:27.305 INFO: Atomic Energies not in training file, using command line argument E0s\n",
            "2024-04-02 10:00:27.305 INFO: Atomic energies: [-13.663181292231226, -1029.2809654211628, -1484.1187695035828, -2042.0330099956639]\n",
            "2024-04-02 10:00:27.957 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
            "2024-04-02 10:00:28.151 INFO: Average number of neighbors: 11.986634254455566\n",
            "2024-04-02 10:00:28.151 INFO: Selected the following outputs: {'energy': True, 'forces': True, 'virials': False, 'stress': False, 'dipoles': False}\n",
            "2024-04-02 10:00:28.151 INFO: Building model\n",
            "2024-04-02 10:00:28.152 INFO: Hidden irreps: 32x0e\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
            "  warnings.warn(\n",
            "2024-04-02 10:00:32.033 INFO: Using stochastic weight averaging (after 0 epochs) with energy weight : 1000.0, forces weight : 100.0 and learning rate : 0.001\n",
            "2024-04-02 10:00:32.150 INFO: ScaleShiftMACE(\n",
            "  (node_embedding): LinearNodeEmbeddingBlock(\n",
            "    (linear): Linear(4x0e -> 32x0e | 128 weights)\n",
            "  )\n",
            "  (radial_embedding): RadialEmbeddingBlock(\n",
            "    (bessel_fn): BesselBasis(r_max=4.0, num_basis=8, trainable=False)\n",
            "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=4.0)\n",
            "  )\n",
            "  (spherical_harmonics): SphericalHarmonics()\n",
            "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-13.6632, -1029.2810, -1484.1188, -2042.0330])\n",
            "  (interactions): ModuleList(\n",
            "    (0-1): 2 x RealAgnosticResidualInteractionBlock(\n",
            "      (linear_up): Linear(32x0e -> 32x0e | 1024 weights)\n",
            "      (conv_tp): TensorProduct(32x0e x 1x0e+1x1o+1x2e+1x3o -> 32x0e+32x1o+32x2e+32x3o | 128 paths | 128 weights)\n",
            "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 128]\n",
            "      (linear): Linear(32x0e+32x1o+32x2e+32x3o -> 32x0e+32x1o+32x2e+32x3o | 4096 weights)\n",
            "      (skip_tp): FullyConnectedTensorProduct(32x0e x 4x0e -> 32x0e | 4096 paths | 4096 weights)\n",
            "      (reshape): reshape_irreps()\n",
            "    )\n",
            "  )\n",
            "  (products): ModuleList(\n",
            "    (0-1): 2 x EquivariantProductBasisBlock(\n",
            "      (symmetric_contractions): SymmetricContraction(\n",
            "        (contractions): ModuleList(\n",
            "          (0): Contraction(\n",
            "            (contractions_weighting): ModuleList(\n",
            "              (0-1): 2 x GraphModule()\n",
            "            )\n",
            "            (contractions_features): ModuleList(\n",
            "              (0-1): 2 x GraphModule()\n",
            "            )\n",
            "            (weights): ParameterList(\n",
            "                (0): Parameter containing: [torch.float32 of size 4x4x32]\n",
            "                (1): Parameter containing: [torch.float32 of size 4x1x32]\n",
            "            )\n",
            "            (graph_opt_main): GraphModule()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (linear): Linear(32x0e -> 32x0e | 1024 weights)\n",
            "    )\n",
            "  )\n",
            "  (readouts): ModuleList(\n",
            "    (0): LinearReadoutBlock(\n",
            "      (linear): Linear(32x0e -> 1x0e | 32 weights)\n",
            "    )\n",
            "    (1): NonLinearReadoutBlock(\n",
            "      (linear_1): Linear(32x0e -> 16x0e | 512 weights)\n",
            "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
            "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
            "    )\n",
            "  )\n",
            "  (scale_shift): ScaleShiftBlock(scale=0.946350, shift=-4.993647)\n",
            ")\n",
            "2024-04-02 10:00:32.153 INFO: Number of parameters: 62128\n",
            "2024-04-02 10:00:32.153 INFO: Optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: True\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    initial_lr: 0.01\n",
            "    lr: 0.01\n",
            "    maximize: False\n",
            "    name: embedding\n",
            "    swa_lr: 0.001\n",
            "    weight_decay: 0.0\n",
            "\n",
            "Parameter Group 1\n",
            "    amsgrad: True\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    initial_lr: 0.01\n",
            "    lr: 0.01\n",
            "    maximize: False\n",
            "    name: interactions_decay\n",
            "    swa_lr: 0.001\n",
            "    weight_decay: 5e-07\n",
            "\n",
            "Parameter Group 2\n",
            "    amsgrad: True\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    initial_lr: 0.01\n",
            "    lr: 0.01\n",
            "    maximize: False\n",
            "    name: interactions_no_decay\n",
            "    swa_lr: 0.001\n",
            "    weight_decay: 0.0\n",
            "\n",
            "Parameter Group 3\n",
            "    amsgrad: True\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    initial_lr: 0.01\n",
            "    lr: 0.01\n",
            "    maximize: False\n",
            "    name: products\n",
            "    swa_lr: 0.001\n",
            "    weight_decay: 5e-07\n",
            "\n",
            "Parameter Group 4\n",
            "    amsgrad: True\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    initial_lr: 0.01\n",
            "    lr: 0.01\n",
            "    maximize: False\n",
            "    name: readouts\n",
            "    swa_lr: 0.001\n",
            "    weight_decay: 0.0\n",
            ")\n",
            "2024-04-02 10:00:32.153 INFO: Using gradient clipping with tolerance=10.000\n",
            "2024-04-02 10:00:32.153 INFO: Started training\n",
            "2024-04-02 10:00:32.153 INFO: Changing loss based on SWA\n",
            "2024-04-02 10:01:02.147 INFO: Epoch 0: loss=44.7949, RMSE_E_per_atom=5.1 meV, RMSE_F=676.0 meV / A\n",
            "2024-04-02 10:01:29.292 INFO: Training complete\n",
            "2024-04-02 10:01:29.294 INFO: Computing metrics for training, validation, and test sets\n",
            "2024-04-02 10:01:29.295 INFO: Loading checkpoint: checkpoints/MACE_model_run-123_epoch-0.pt\n",
            "2024-04-02 10:01:29.309 INFO: Loaded model from epoch 0\n",
            "2024-04-02 10:01:29.805 INFO: Evaluating train ...\n",
            "2024-04-02 10:01:39.870 INFO: Evaluating valid ...\n",
            "2024-04-02 10:01:42.821 INFO: Evaluating Default ...\n",
            "2024-04-02 10:02:19.350 INFO: \n",
            "+-------------+---------------------+------------------+-------------------+\n",
            "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
            "+-------------+---------------------+------------------+-------------------+\n",
            "|    train    |         6.1         |      687.5       |       72.64       |\n",
            "|    valid    |         5.1         |      676.0       |       69.16       |\n",
            "|   Default   |         6.1         |      693.5       |       72.19       |\n",
            "+-------------+---------------------+------------------+-------------------+\n",
            "2024-04-02 10:02:19.350 INFO: Saving model to checkpoints/MACE_model_run-123.model\n",
            "2024-04-02 10:02:19.457 WARNING: No SWA checkpoint found, while SWA is enabled. Compare the swa_start parameter and the latest checkpoint.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/./mace/scripts/run_train.py\", line 6, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mace/cli/run_train.py\", line 521, in main\n",
            "    epoch = checkpoint_handler.load_latest(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mace/tools/checkpoint.py\", line 210, in load_latest\n",
            "    result = self.io.load_latest(swa=swa, device=device)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mace/tools/checkpoint.py\", line 171, in load_latest\n",
            "    path = self._get_latest_checkpoint_path(swa=swa)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mace/tools/checkpoint.py\", line 152, in _get_latest_checkpoint_path\n",
            "    return latest_checkpoint_info.path\n",
            "UnboundLocalError: local variable 'latest_checkpoint_info' referenced before assignment\n"
          ]
        }
      ],
      "source": [
        "!python3 ./mace/scripts/run_train.py \\\n",
        "  --name=\"MACE_model\" \\\n",
        "  --train_file=\"BOTNet-datasets/dataset_3BPA/train_300K.xyz\" \\\n",
        "  --valid_fraction=0.05 \\\n",
        "  --test_file=\"BOTNet-datasets/dataset_3BPA/test_300K.xyz\" \\\n",
        "  --E0s='{1:-13.663181292231226, 6:-1029.2809654211628, 7:-1484.1187695035828, 8:-2042.0330099956639}' \\\n",
        "  --model=\"ScaleShiftMACE\" \\\n",
        "  --hidden_irreps='32x0e' \\\n",
        "  --r_max=4.0 \\\n",
        "  --batch_size=20 \\\n",
        "  --max_num_epochs=100 \\\n",
        "  --ema \\\n",
        "  --ema_decay=0.99 \\\n",
        "  --amsgrad \\\n",
        "  --default_dtype=\"float32\" \\\n",
        "  --device=cpu \\\n",
        "  --seed=123 \\\n",
        "  --swa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXeQe8oTPXQ2"
      },
      "source": [
        "It is possible to use `--model=MACE`, in order to have the correct limit for isolated atoms. This is recommanded for task studying bond breaking events."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ln2lu5i4aWA"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWKzaxlA2dow"
      },
      "source": [
        "The trained model is realidy usable to run some ASE MD for illustration. The Colab hardware are not very performant so we put a small number of timesteps for illustration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfCwdnaWv9rd"
      },
      "outputs": [],
      "source": [
        "from ase import units\n",
        "from ase.md.langevin import Langevin\n",
        "from ase.io import read, write\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from mace.calculators import MACECalculator\n",
        "\n",
        "calculator = MACECalculator(model_paths='/content/checkpoints/MACE_model_run-123.model', device='cpu')\n",
        "init_conf = read('BOTNet-datasets/dataset_3BPA/test_300K.xyz', '0')\n",
        "init_conf.set_calculator(calculator)\n",
        "\n",
        "dyn = Langevin(init_conf, 0.5*units.fs, temperature_K=310, friction=5e-3)\n",
        "def write_frame():\n",
        "        dyn.atoms.write('md_3bpa.xyz', append=True)\n",
        "dyn.attach(write_frame, interval=50)\n",
        "dyn.run(100)\n",
        "print(\"MD finished!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
