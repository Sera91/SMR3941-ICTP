{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaflxNZCWJc1"
      },
      "source": [
        "# Coding a Transformer from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_fSAUpAZj9E"
      },
      "source": [
        "### Cristiano De Nobili - My Contacts\n",
        "For any questions or doubts you can find my contacts here:\n",
        "\n",
        "<p align=\"center\">\n",
        "\n",
        "[<img src=\"https://img.freepik.com/premium-vector/linkedin-logo_578229-227.jpg?w=1060\" width=\"25\">](https://www.linkedin.com/in/cristiano-de-nobili/) [<img src=\"https://1.bp.blogspot.com/-Rwqcet_SHbk/T8_acMUmlmI/AAAAAAAAGgw/KD_fx__8Q4w/s1600/Twitter+bird.png\" width=\"30\">](https://twitter.com/denocris)        \n",
        "\n",
        "</p>\n",
        "\n",
        "or here (https://denocris.com).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xyzbEsEso4e"
      },
      "source": [
        "\n",
        "\n",
        "Some refs:\n",
        "\n",
        "* Original Paper: [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf);\n",
        "\n",
        "* [The Illustrated Transformer - Jay Alammar.](http://jalammar.github.io/illustrated-transformer/)\n",
        "\n",
        "\n",
        "Disclaimers:\n",
        "\n",
        "We will train in a standard way a Transformer Model. This is not BERT, which is a collection of Transformer layers. BERT is trained according to the Masked Language Model (MLM) paradigm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bjO3CSxXrfw"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install torchtext==0.6.0\n",
        "\n",
        "# After the installation, restart the session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7tW-onneT6e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "#from torchtext.legacy import data, datasets\n",
        "\n",
        "from torchtext import data, datasets\n",
        "from torchtext import vocab\n",
        "import numpy as np\n",
        "import random, tqdm, sys, math, gzip\n",
        "from torchsummary import summary\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fV_Q7l8USGW"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL2RYfclXJCC",
        "outputId": "6b92df56-5a34-4044-b96b-48a167fbb22d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon May 20 11:41:12 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8              11W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8AJr_0OWts0"
      },
      "source": [
        "### Multi-head Attention Mechanism, step by step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fU9jf6fW6k5"
      },
      "source": [
        "First, let's set some hyperparameters. To keep it simple we choose small size hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbxZtfxmW5gJ"
      },
      "outputs": [],
      "source": [
        "emb = 128 # embedding dimension (BERT like models 768)\n",
        "h = 8 # number of heads (BERT has 12 heads)\n",
        "\n",
        "batch_size = 4\n",
        "sentence_length = 21 # Context Length, 512 for BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H_vTfvzXPoy"
      },
      "source": [
        "Some fake random data with proper dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGxzWZhAXQCj"
      },
      "outputs": [],
      "source": [
        "x = torch.rand(batch_size, 21, emb)\n",
        "\n",
        "b, t, e = x.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qvIpIkVYb2f",
        "outputId": "7c459de5-f7c9-4c17-e656-a4a0467e8289"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 21, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "x.size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL1idC2yYKIg"
      },
      "source": [
        "Instantiate linear transformations for query, key and values. Each transformation will act on the input vector x."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pg5YyRrfY5b5"
      },
      "outputs": [],
      "source": [
        "tokeys    = nn.Linear(emb, emb, bias=False) # W_key\n",
        "toqueries = nn.Linear(emb, emb, bias=False) # W_query\n",
        "tovalues  = nn.Linear(emb, emb, bias=False) # W_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oUigFk1ZDNV"
      },
      "source": [
        "Generate queries, keys and values. We first compute the k/q/v's on the whole embedding vectors, and then split into the different heads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3w9dbk45Y9k2"
      },
      "outputs": [],
      "source": [
        "keys    = tokeys(x) # W_key x\n",
        "queries = toqueries(x)\n",
        "values  = tovalues(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NItFD41X2ZqM",
        "outputId": "9df81fbe-22d5-41b8-8bfe-25e3d408bd78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 21, 128])\n"
          ]
        }
      ],
      "source": [
        "print(keys.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfknxyThZxk3"
      },
      "source": [
        "Implement now multi-head attention (the ligther version), splitting into the different heads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgDinTEcZx-R",
        "outputId": "36615915-190c-40af-d64e-8c9b88cd35b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 21, 8, 16])\n"
          ]
        }
      ],
      "source": [
        "s = e // h # 128 / 8\n",
        "\n",
        "keys    = keys.view(b, t, h, s)\n",
        "queries = queries.view(b, t, h, s)\n",
        "values  = values.view(b, t, h, s)\n",
        "\n",
        "print(keys.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIkZDhKdf9Qs",
        "outputId": "938b075e-b141-4e52-b881-d7fa2fe10426"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 21, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "keys.transpose(1, 2).size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yQO6hUrbztU",
        "outputId": "f3bc55a2-9f74-42d8-b181-f1950fa74f89"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 21, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "keys.transpose(1, 2).contiguous().view(b * h, t, s).size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bArBBpHJdizO"
      },
      "source": [
        "We need now to compute the dot products. This is the same operation for every head, so we fold the heads into the batch dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVHJlXNWgLJc",
        "outputId": "0b047e0d-eaf7-44e2-a7e2-b15d1a81b894"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 21, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "keys = keys.transpose(1, 2).contiguous().view(b * h, t, s)\n",
        "queries = queries.transpose(1, 2).contiguous().view(b * h, t, s)\n",
        "values = values.transpose(1, 2).contiguous().view(b * h, t, s)\n",
        "\n",
        "# contiguous():  it actually makes a copy of the tensor such that the order of\n",
        "# its elements in memory is the same as if it had been created from scratch with the same data.\n",
        "# transpose(1, 2) doesn't generate a new tensor with a new layout, it just\n",
        "# modifies meta information in the Tensor object so that the offset and stride describe the desired new shape.\n",
        "# https://discuss.pytorch.org/t/contigious-vs-non-contigious-tensor/30107\n",
        "\n",
        "keys.size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RS1sy_-e-Rd"
      },
      "source": [
        "Perform dot products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzFcqa4Sx2G4",
        "outputId": "72247253-9d6d-4dd8-b79e-653f138f7533"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 21, 16])\n",
            "torch.Size([32, 16, 21])\n",
            "torch.Size([32, 21, 21])\n"
          ]
        }
      ],
      "source": [
        "print(queries.size())\n",
        "\n",
        "print(keys.transpose(1, 2).size())\n",
        "\n",
        "#Let's compute the attention matrix\n",
        "attn_scores = torch.bmm(queries, keys.transpose(1, 2)).size()  # batch matrix-matrix product\n",
        "\n",
        "print(attn_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ’¡ Note: `torch.bmm` and the symbol `@` are the same thing. You can check easily:\n",
        "\n",
        "```\n",
        "mat1 = torch.randn(10, 3, 4)\n",
        "mat2 = torch.randn(10, 4, 5)\n",
        "res_1 = torch.bmm(mat1, mat2)\n",
        "res_2 = mat1 @ mat2\n",
        "```\n",
        "In particular:\n",
        "* `torch.bmm`: Performs a batch matrix-matrix product of 3D tensors `mat1.size() = [b,n,m]` and `mat2.size() = [b,m,p]`, outputting `res.size() = [b,n,p]`. Here is the [documentation](https://pytorch.org/docs/stable/generated/torch.bmm.html#torch.bmm).\n",
        "* symbol `@`: The matrix multiplication(s) are done between the last two dimensions. The remaining dimensions are broadcast and batched.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YU9ReHCw2IQ5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFlUkxtLfQml"
      },
      "source": [
        "Just for completeness, below the implementation of the original multi-head attention (which is wide and computationally more intensive)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkNaNkPH9eVY",
        "outputId": "1fdd098a-b0f0-4fd0-8a7e-ccd79749fff4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 21, 1024])\n",
            "torch.Size([4, 21, 8, 128])\n"
          ]
        }
      ],
      "source": [
        "emb = 128\n",
        "h = 8\n",
        "\n",
        "x = torch.rand(4, 21, emb)\n",
        "\n",
        "b, t, e = x.size()\n",
        "\n",
        "tokeys    = nn.Linear(emb, emb * h, bias=False)\n",
        "toqueries = nn.Linear(emb, emb * h, bias=False)\n",
        "tovalues  = nn.Linear(emb, emb * h, bias=False)\n",
        "\n",
        "keys    = tokeys(x)\n",
        "queries = toqueries(x)\n",
        "values  = tovalues(x)\n",
        "\n",
        "print(keys.size())\n",
        "\n",
        "keys    = keys.view(b, t, h, e)\n",
        "queries = queries.view(b, t, h, e)\n",
        "values  = values.view(b, t, h, e)\n",
        "\n",
        "print(keys.size())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DC9fbHbnzhP"
      },
      "source": [
        "### Model Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPaxkMJaffkh"
      },
      "source": [
        "Let us collect everything and define the self-attention class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrvYJ5tPYI7L"
      },
      "outputs": [],
      "source": [
        "class MHSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head self attention.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, emb, heads=8):\n",
        "        \"\"\"\n",
        "        :param emb:\n",
        "        :param heads:\n",
        "        :param mask:\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        assert emb % heads == 0, f'Embedding dimension ({emb}) should be divisible by nr. of heads ({heads})'\n",
        "\n",
        "        self.emb = emb\n",
        "        self.heads = heads\n",
        "\n",
        "        #s = emb // heads\n",
        "        # - We will break the embedding into `heads` chunks and feed each to a different attention head\n",
        "\n",
        "        self.tokeys    = nn.Linear(emb, emb, bias=False) # W_key\n",
        "        self.toqueries = nn.Linear(emb, emb, bias=False) # W_query\n",
        "        self.tovalues  = nn.Linear(emb, emb, bias=False) # W_value\n",
        "\n",
        "        self.unifyheads = nn.Linear(emb, emb)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        b, t, e = x.size()\n",
        "        h = self.heads\n",
        "        assert e == self.emb, f'Input embedding dim ({e}) should match layer embedding dim ({self.emb})'\n",
        "\n",
        "        s = e // h\n",
        "\n",
        "        # We first compute the k/q/v's on the whole embedding vectors, and then split into the different heads.\n",
        "\n",
        "        keys    = self.tokeys(x)\n",
        "        queries = self.toqueries(x)\n",
        "        values  = self.tovalues(x)\n",
        "\n",
        "        # Split into the different heads.\n",
        "\n",
        "        keys    = keys.view(b, t, h, s)\n",
        "        queries = queries.view(b, t, h, s)\n",
        "        values  = values.view(b, t, h, s)\n",
        "\n",
        "        # Compute scaled dot-product self-attention\n",
        "\n",
        "        # Fold heads into the batch dimension\n",
        "        # When you call contiguous(), it actually makes a copy of the tensor\n",
        "        # such that the order of its elements in memory is the same as if it had been created from scratch with the same data.\n",
        "        keys = keys.transpose(1, 2).contiguous().view(b * h, t, s)\n",
        "        queries = queries.transpose(1, 2).contiguous().view(b * h, t, s)\n",
        "        values = values.transpose(1, 2).contiguous().view(b * h, t, s)\n",
        "\n",
        "        queries = queries / (e ** (1/4))\n",
        "        keys    = keys / (e ** (1/4))\n",
        "        # Instead of dividing the dot products by sqrt(e), we scale the keys and values.\n",
        "        # This should be more memory efficient\n",
        "\n",
        "        # Get dot product of queries and keys, and scale.\n",
        "\n",
        "        attn_scores = torch.bmm(queries, keys.transpose(1, 2))\n",
        "\n",
        "        assert attn_scores.size() == (b * h, t, t)\n",
        "\n",
        "        attn_weights = F.softmax(attn_scores, dim=2) # Dot now has row-wise self-attention probabilities\n",
        "\n",
        "        # apply the self attention to the values\n",
        "        out = torch.bmm(attn_weights, values).view(b, h, t, s)\n",
        "\n",
        "        # swap h, t back, unify heads\n",
        "        out = out.transpose(1, 2).contiguous().view(b, t, s * h)\n",
        "\n",
        "        return self.unifyheads(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIVMd_jgoTUG"
      },
      "source": [
        "A Transformer Block is based on self-attention (and Layer Normalization, Residual Connections)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edyQsYXSOe7j"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, emb, heads, mask, seq_length, ff_hidden_mult=4, dropout=0.0, pos_embedding=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mhattention = MHSelfAttention(emb, heads=heads)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(emb)\n",
        "        self.norm2 = nn.LayerNorm(emb)\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(emb, ff_hidden_mult * emb),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_hidden_mult * emb, emb)\n",
        "        )\n",
        "\n",
        "        self.do = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        attended = self.mhattention(x)\n",
        "\n",
        "        x = self.norm1(attended + x) #residual\n",
        "\n",
        "        x = self.do(x)\n",
        "\n",
        "        fedforward = self.ff(x)\n",
        "\n",
        "        x = self.norm2(fedforward + x) #residual\n",
        "\n",
        "        x = self.do(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLkJs-d1oo4o"
      },
      "source": [
        "Let's build a Transformers (a stack of Transformers Blocks) and adapt it for a binary classification task. Its `depth` defines the number of Transformers Blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOEjyiOKGyYO"
      },
      "outputs": [],
      "source": [
        "class CTransformer(nn.Module):\n",
        "\n",
        "    def __init__(self, emb, heads, depth, seq_length, num_tokens, num_classes, max_pool=True, dropout=0.0):\n",
        "        \"\"\"\n",
        "        :param emb: Embedding dimension\n",
        "        :param heads: nr. of attention heads\n",
        "        :param depth: Number of transformer blocks\n",
        "        :param seq_length: Expected maximum sequence length\n",
        "        :param num_tokens: Number of tokens (usually words) in the vocabulary\n",
        "        :param num_classes: Number of classes.\n",
        "        :param max_pool: If true, use global max pooling in the last layer. If false, use global\n",
        "                         average pooling.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_tokens, self.max_pool = num_tokens, max_pool\n",
        "\n",
        "        # Token embedding\n",
        "        self.token_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=num_tokens)\n",
        "        # Position embedding\n",
        "        self.pos_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=seq_length)\n",
        "\n",
        "        tblocks = []\n",
        "        for i in range(depth):\n",
        "            tblocks.append(\n",
        "                TransformerBlock(emb=emb, heads=heads, seq_length=seq_length, mask=False, dropout=dropout))\n",
        "\n",
        "        self.tblocks = nn.Sequential(*tblocks)\n",
        "\n",
        "        self.toprobs = nn.Linear(emb, num_classes)\n",
        "\n",
        "        self.do = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: A batch by sequence length integer tensor of token indices.\n",
        "        :return: predicted log-probability vectors for each token based on the preceding tokens.\n",
        "        \"\"\"\n",
        "        tokens = self.token_embedding(x)\n",
        "        b, t, e = tokens.size()\n",
        "\n",
        "        positions = self.pos_embedding(torch.arange(t, device=device))[None, :, :].expand(b, t, e)\n",
        "        x = tokens + positions\n",
        "        x = self.do(x)\n",
        "\n",
        "        x = self.tblocks(x)\n",
        "\n",
        "        x = x.max(dim=1)[0] if self.max_pool else x.mean(dim=1) # pool over the time dimension\n",
        "\n",
        "        x = self.toprobs(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1) # nn.softmax()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAt7m7h-1jxH",
        "outputId": "336e38ff-6815-4466-c327-3a77e29941f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
              "        18, 19, 20])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "torch.arange(21)\n",
        "# [0, 0, ..., 0]\n",
        "# [1, 1, ..., 1]\n",
        "# ...\n",
        "# [20, 20, ..., 20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLHykvt51B9Q"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUq9RpF-1M1L"
      },
      "source": [
        "One of the main concepts of TorchText is the `Field`. These define how your data should be processed. In our sentiment classification task the data consists of both the raw string of the review and the sentiment, either \"pos\" or \"neg\".\n",
        "\n",
        "The parameters of a `Field` specify how the data should be processed.\n",
        "\n",
        "We use the `TEXT` field to define how the review should be processed, and the `LABEL` field to process the sentiment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwZCH0cdNNld"
      },
      "outputs": [],
      "source": [
        "TEXT = data.Field(lower=True, include_lengths=True, batch_first=True) # If no tokenize argument is passed, the default is simply splitting the string on spaces.\n",
        "LABEL = data.Field(sequential=False)\n",
        "\n",
        "NUM_CLS = 2\n",
        "BATCH_SIZE = 4\n",
        "MAX_LENGTH = 256 #512\n",
        "EMB_SIZE = 128\n",
        "HEADS = 8\n",
        "DEPTH = 3 #Number of self-attention layer\n",
        "VOC_SIZE = 50000\n",
        "\n",
        "LR_RATE = 0.0001\n",
        "WARMUP = 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1MIFNoQXyJO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44f74f4c-eb8e-4d42-f107-bb105e31a9ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "aclImdb_v1.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84.1M/84.1M [00:14<00:00, 5.93MB/s]\n"
          ]
        }
      ],
      "source": [
        "tbw = SummaryWriter(log_dir='./logs') # Tensorboard logging\n",
        "\n",
        "train, test = datasets.IMDB.splits(TEXT, LABEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwosp3eD2DKz",
        "outputId": "9e35279d-3a90-434c-d1c0-961a799594f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': ['first', 'of', 'all,', 'i', 'loved', 'bruce', \"broughton's\", 'music', 'score,', 'very', 'lyrical,', 'and', 'this', 'alone', 'added', 'to', 'the', \"film's\", 'charm.', 'the', 'best', 'aspect', 'of', 'the', 'movie', 'were', 'the', 'three', 'animals,', 'superlatively', 'voiced', 'by', 'michael', 'j.fox,', 'sally', 'field', 'and', 'the', 'late', 'don', 'ameche.', 'whereas', 'fox', 'has', 'the', 'funniest', 'lines,', 'ameche', 'plays', 'a', 'rather', 'brooding', 'otherwise', 'engaging', 'character(the', 'voice', 'of', 'reason),', 'and', 'field', 'adds', 'wit', 'into', 'a', 'character', 'that', 'is', 'always', 'seen', 'telling', 'chance', 'off.', 'the', 'humans', \"weren't\", 'as', 'engaging,', 'and', 'sometimes', 'the', 'film', 'dragged,', 'but', 'that', 'is', 'my', 'only', 'complaint.', 'this', 'is', 'one', 'beautiful-looking', 'film,', 'with', 'beautiful', 'close', 'up', 'shots', 'of', 'canada,', 'i', 'believe.', 'although', 'the', 'film', 'itself', 'is', 'quite', 'long,', 'there', 'is', 'never', 'a', 'seriously', 'dull', 'moment,', 'and', 'this', 'is', 'advantaged', 'by', 'the', 'voice', 'work', 'and', 'a', 'well-written', 'script.', 'all', 'in', 'all,', 'a', 'charming', 'and', 'perhaps', 'underrated', 'film,', 'with', 'a', '9/10', 'from', 'me.', 'bethany', 'cox.'], 'label': 'pos'}\n"
          ]
        }
      ],
      "source": [
        "# View 'text' and 'label'\n",
        "print(vars(train.examples[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M67EEOAjO6Yw"
      },
      "outputs": [],
      "source": [
        "TEXT.build_vocab(train, max_size=VOC_SIZE - 2)\n",
        "LABEL.build_vocab(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaIjffZVSRTM"
      },
      "outputs": [],
      "source": [
        "train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=BATCH_SIZE, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nddhgBgyfGo",
        "outputId": "9f750b70-b916-4c75-9622-4552ce31c5b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  292,    43,    10,    24,     7, 18489,     0,  7672,   590,   552,\n",
            "          3394,  2064,  2133,  5502,    10,    20,  9396,    72,     0,     2,\n",
            "           201,   207,    26,    71,     3,  3717,     0,   643,  3500,     7,\n",
            "           421,   425,     4,    41,    23,   176,     3,  2657,    19,    48,\n",
            "             7,   162,    19,   411,     2,  1321,   405,    22,  4379,    60,\n",
            "           132,   883,    29,   364,    49,   467,     0,     4,    39,   214,\n",
            "           245,     0,   301,   362,    22,    16,     2, 47820,     2,    38,\n",
            "           437,  2775,  1352,   197,    10,  2336,  3498,     8,  3490,     0,\n",
            "            17,   467,   275,  7770,     0, 25170,    11,    22,     0,     4,\n",
            "             3,  1276,  2121,    11,  4285,    17,     3,  2264,     5,  4825,\n",
            "         12880,    46,     2,  2336,     7,   443,  5745,     4,   164,     0,\n",
            "           144,  1587, 13608,  1214, 24076,   313,   292,     7, 16132,     2,\n",
            "           999,   128,    28,     2,    82,  2039,     5,     2,  1454,   360,\n",
            "            17,     2,  1280,     4,     2,  8304,     2,   211,    24,  3498,\n",
            "             8,     3,     0,  6813,   339,     6,   167,  1716,    16,  4141,\n",
            "            48,     3,  4625, 13916,   128,    26,     6,   110,   471,    38,\n",
            "            33,    64,  3757,  6903,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1],\n",
            "        [   10,    24,     7,     0,    12,     7,    51,  1036,    96,     3,\n",
            "         14277,     0, 18660,    50,    27,     0,     2,  2692,    16,     2,\n",
            "             0,    30,   200,     2,   336,   577,    65,  2801,   140,     2,\n",
            "           139,     2,  3433,  1647,    87,    50,     9,    14,   149, 21411,\n",
            "           100,    89,   118,    50,     2,  5285,     7,  1167,     9,   441,\n",
            "            10,   696,    77,     5,     2,   590,    18,     2,    75,     7,\n",
            "          4519,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1],\n",
            "        [   48,   549,    14,   162,    19,   276,   234,   408,  3059,     8,\n",
            "           154, 14840,    50,   299,  2058,    66,     0,    13,   255,   552,\n",
            "            75,     5,   923,  3368,    14,    61,  1658,     6,  4376,    64,\n",
            "            25,   866,  1061,  1906,  7763,    15,     2,   951,  1026,     6,\n",
            "          5652,     0,  7763,   525,    51,    37,    42,  3948,    55,  3645,\n",
            "            14,    11,  7763,   398,  3485,     8,     2,   749,    13,     0,\n",
            "          8489,     4,   775, 10449,    36,  1104,   130,    88,    19,   387,\n",
            "          1235,   923,   181,   370,     8,   222,     0,    26,    60,   159,\n",
            "           373,     8,    10,   139,  8489,   270,   167,  1026,    36,   254,\n",
            "           137,     6, 35542,     6, 11053,   197,  1001,   446,     2,   231,\n",
            "            11,    53,     7, 12171,     0,     6,    42,    50,    42,   826,\n",
            "           239,   149,    34,     2,  9833,    13, 27456,  6900,     7,  8868,\n",
            "             4,   554,  1212,    17,     3,    76, 12596,   775, 26580,    52,\n",
            "             7,    55,  1212,    16,     2,   185,   741,  6388,  2984,    13,\n",
            "            93,   542,    41,   105,     6,    60,  9158,    22,    33,   243,\n",
            "             6,   142,    11,    35,     7,  1497,     8,   120,     4, 22178,\n",
            "            46,    33,  1929,    33,   113,     3,   350,   331,     8,  4172,\n",
            "          2723,    13,    93,  2389,     5,   722,    17, 10449,     4,  6900,\n",
            "             7,  1048, 19237,  2521,    11,     7,    48,   128,    26,    71,\n",
            "          1048,   247,     6,    10,  1859,  1873,    24,     5, 29809,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1],\n",
            "        [   99,     3,    24,  7431, 10910,     9,    54,    37,     6,    98,\n",
            "            11,     9,    26,   119,     3,  7613,  2456,     5,   581,    45,\n",
            "            56,     4,    45,   665,    18,     9,    54,    26,     6,   142,\n",
            "            11,     0,     0,     7,    31,   230,     2,   572,    24,     9,\n",
            "            26,   125,  8388,     9,  2433,     2,    24,     8,     2,   418,\n",
            "            11,    12,    14,   162,     6,    28,     3,    88,   632,   409,\n",
            "           130,    15,     0,     4,     0,    40,     0,    18,   819,    60,\n",
            "            10,    14,    38,    97,    11,    57,  1367, 11508,     4,     9,\n",
            "          1154,    65,  1218,     4, 11160,    29,     2,   895,   132,     4,\n",
            "          1100,   272,     0,    87,     5,     3,    97,     0,     8,     0,\n",
            "           466,     0,     2,   258,   121,  3360,     4,  2228,  1769,     6,\n",
            "          8423,   188,    34,     2,  4020,     0,     2,   258,   122,    36,\n",
            "           517,    17,    42,   103,   440,    41,     2,   585,     5,   469,\n",
            "            11,    25,    54,   102,    73,  1505,     5, 13748,     4,  2228,\n",
            "            14,    38,  5197,  2510,   719,  2735,   214,     8,  2151,   158,\n",
            "           845, 14633,    11,    31,     2,   172,     5,     2,    24,     9,\n",
            "            61,   147,   184,     3,  1561,    43,   452,     5,   429,    35,\n",
            "             9,    90,    98,    14, 23736,     9,  1509,     0,    16,    10,\n",
            "             0,     9,    26,   102,   459,     3,   882,   179,    18,    94,\n",
            "           148,    10,    24,    12,    41, 45253,    87,    19,     6,  3027,\n",
            "            89,     5,    10,  6947,  2466,    38,   785,   909,    29,    35,\n",
            "          3962,  1286,    16, 18727]], device='cuda:0')\n",
            "tensor([0, 0, 0, 0], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# batch size = 4\n",
        "for batch in train_iter:\n",
        "\n",
        "    input = batch.text[0]\n",
        "    label = batch.label - 1\n",
        "\n",
        "    print(input)\n",
        "    print(label)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLvUJVGgSYu9",
        "outputId": "9dbdd34a-450b-4586-c1a7-94bb0a79d522"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- nr. of training examples 6250\n",
            "- nr. of test examples 6250\n"
          ]
        }
      ],
      "source": [
        "print(f'- nr. of training examples {len(train_iter)}')\n",
        "print(f'- nr. of test examples {len(test_iter)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMi5jQ5fV8l7",
        "outputId": "8c3408e1-dfa9-443d-aaac-b75fdfa0a2fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CTransformer(\n",
              "  (token_embedding): Embedding(50000, 128)\n",
              "  (pos_embedding): Embedding(256, 128)\n",
              "  (tblocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (mhattention): MHSelfAttention(\n",
              "        (tokeys): Linear(in_features=128, out_features=128, bias=False)\n",
              "        (toqueries): Linear(in_features=128, out_features=128, bias=False)\n",
              "        (tovalues): Linear(in_features=128, out_features=128, bias=False)\n",
              "        (unifyheads): Linear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ff): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (1): ReLU()\n",
              "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "      )\n",
              "      (do): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (mhattention): MHSelfAttention(\n",
              "        (tokeys): Linear(in_features=128, out_features=128, bias=False)\n",
              "        (toqueries): Linear(in_features=128, out_features=128, bias=False)\n",
              "        (tovalues): Linear(in_features=128, out_features=128, bias=False)\n",
              "        (unifyheads): Linear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ff): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (1): ReLU()\n",
              "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "      )\n",
              "      (do): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (mhattention): MHSelfAttention(\n",
              "        (tokeys): Linear(in_features=128, out_features=128, bias=False)\n",
              "        (toqueries): Linear(in_features=128, out_features=128, bias=False)\n",
              "        (tovalues): Linear(in_features=128, out_features=128, bias=False)\n",
              "        (unifyheads): Linear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ff): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (1): ReLU()\n",
              "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "      )\n",
              "      (do): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (mhattention): MHSelfAttention(\n",
              "        (tokeys): Linear(in_features=128, out_features=128, bias=False)\n",
              "        (toqueries): Linear(in_features=128, out_features=128, bias=False)\n",
              "        (tovalues): Linear(in_features=128, out_features=128, bias=False)\n",
              "        (unifyheads): Linear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ff): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (1): ReLU()\n",
              "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "      )\n",
              "      (do): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (mhattention): MHSelfAttention(\n",
              "        (tokeys): Linear(in_features=128, out_features=128, bias=False)\n",
              "        (toqueries): Linear(in_features=128, out_features=128, bias=False)\n",
              "        (tovalues): Linear(in_features=128, out_features=128, bias=False)\n",
              "        (unifyheads): Linear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ff): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (1): ReLU()\n",
              "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "      )\n",
              "      (do): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (mhattention): MHSelfAttention(\n",
              "        (tokeys): Linear(in_features=128, out_features=128, bias=False)\n",
              "        (toqueries): Linear(in_features=128, out_features=128, bias=False)\n",
              "        (tovalues): Linear(in_features=128, out_features=128, bias=False)\n",
              "        (unifyheads): Linear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ff): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (1): ReLU()\n",
              "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "      )\n",
              "      (do): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (toprobs): Linear(in_features=128, out_features=2, bias=True)\n",
              "  (do): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# create the model\n",
        "model = CTransformer(emb=EMB_SIZE, heads=HEADS, depth=DEPTH, seq_length=MAX_LENGTH, num_tokens=VOC_SIZE, num_classes=NUM_CLS, max_pool=\"store_true\", dropout=0.2)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNIoieCfcbpa"
      },
      "outputs": [],
      "source": [
        "opt = torch.optim.Adam(lr=LR_RATE, params=model.parameters())\n",
        "sch = torch.optim.lr_scheduler.LambdaLR(opt, lambda i: min(i / (WARMUP / BATCH_SIZE), 1.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "f2WjYN9If758",
        "outputId": "4beacb6d-3e8e-460a-9c1f-4c5429fa0073"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6250/6250 [01:48<00:00, 57.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- test accuracy 0.577\n",
            "\n",
            " epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 5545/6250 [01:34<00:12, 58.61it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-448d378de60e>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Performs gradient clipping. It is used to mitigate the problem of exploding gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# - If the total gradient vector has a length > 1, we clip it back down to 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mmax_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mnorm_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mmax_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mnorm_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mparameters\u001b[0;34m(self, recurse)\u001b[0m\n\u001b[1;32m   2191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2192\u001b[0m         \"\"\"\n\u001b[0;32m-> 2193\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2194\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_parameters\u001b[0;34m(self, prefix, recurse, remove_duplicate)\u001b[0m\n\u001b[1;32m   2224\u001b[0m             \u001b[0;32mlambda\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2225\u001b[0m             prefix=prefix, recurse=recurse, remove_duplicate=remove_duplicate)\n\u001b[0;32m-> 2226\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_named_members\u001b[0;34m(self, get_members_fn, prefix, recurse, remove_duplicate)\u001b[0m\n\u001b[1;32m   2159\u001b[0m         \u001b[0mmemo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m         \u001b[0mmodules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremove_duplicate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2161\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2162\u001b[0m             \u001b[0mmembers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_members_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2371\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2372\u001b[0m                 \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2373\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2375\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2371\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2372\u001b[0m                 \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2373\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2375\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2371\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2372\u001b[0m                 \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2373\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2375\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2371\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2372\u001b[0m                 \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2373\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2375\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 4\n",
        "\n",
        "# training loop\n",
        "seen = 0\n",
        "for e in range(NUM_EPOCHS):\n",
        "\n",
        "    print(f'\\n epoch {e}')\n",
        "    model.train()\n",
        "\n",
        "    for batch in tqdm.tqdm(train_iter):\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        input = batch.text[0]\n",
        "        label = batch.label - 1\n",
        "\n",
        "        if input.size(1) > MAX_LENGTH:\n",
        "            input = input[:, :MAX_LENGTH]\n",
        "\n",
        "\n",
        "        out = model(input)\n",
        "        loss = F.nll_loss(out, label)\n",
        "        # loss = CrossEntropy(out, label)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # clip gradients\n",
        "        # Performs gradient clipping. It is used to mitigate the problem of exploding gradients.\n",
        "        # - If the total gradient vector has a length > 1, we clip it back down to 1.\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        opt.step()\n",
        "        sch.step()\n",
        "\n",
        "        seen += input.size(0)\n",
        "        tbw.add_scalar('classification/train-loss', float(loss.item()), seen)\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        model.eval()\n",
        "        tot, cor= 0.0, 0.0\n",
        "\n",
        "        for batch in test_iter:\n",
        "\n",
        "            input = batch.text[0]\n",
        "            label = batch.label - 1\n",
        "\n",
        "            if input.size(1) > MAX_LENGTH:\n",
        "                input = input[:, :MAX_LENGTH]\n",
        "            out = model(input).argmax(dim=1)\n",
        "\n",
        "            tot += float(input.size(0))\n",
        "            cor += float((label == out).sum().item())\n",
        "\n",
        "        acc = cor / tot\n",
        "        print(f'-- test accuracy {acc:.3}')\n",
        "        tbw.add_scalar('classification/test-loss', float(loss.item()), e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wr1VbzzEn-PH"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "\n",
        "\n",
        "model_inference = model.load_weigths('path')\n",
        "\n",
        "model_inference(\"I bought that book and I enjoyed the readings\")\n",
        "\n",
        "# 0 o 1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}